@book{dunteman_principal_2019,
	location = {Newbury Park, California},
	title = {Principal Components Analysis},
	url = {https://methods.sagepub.com/book/principal-components-analysis},
	author = {Dunteman, George},
	date = {2019-04-30},
	doi = {10.4135/9781412985475}
}

@article{yosinski_understanding_2015,
	title = {Understanding Neural Networks Through Deep Visualization},
	url = {http://arxiv.org/abs/1506.06579},
	abstract = {Recent years have produced great advances in training large, deep neural networks ({DNNs}), including notable successes in training convolutional neural networks (convnets) to recognize natural images. However, our understanding of how these models work, especially what computations they perform at intermediate layers, has lagged behind. Progress in the ﬁeld will be further accelerated by the development of better tools for visualizing and interpreting neural nets. We introduce two such tools here. The ﬁrst is a tool that visualizes the activations produced on each layer of a trained convnet as it processes an image or video (e.g. a live webcam stream). We have found that looking at live activations that change in response to user input helps build valuable intuitions about how convnets work. The second tool enables visualizing features at each layer of a {DNN} via regularized optimization in image space. Because previous versions of this idea produced less recognizable images, here we introduce several new regularization methods that combine to produce qualitatively clearer, more interpretable visualizations. Both tools are open source and work on a pretrained convnet with minimal setup.},
	journaltitle = {{arXiv}:1506.06579 [cs]},
	author = {Yosinski, Jason and Clune, Jeff and Nguyen, Anh and Fuchs, Thomas and Lipson, Hod},
	urldate = {2019-04-19},
	date = {2015-06-22},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1506.06579},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	file = {Yosinski et al. - 2015 - Understanding Neural Networks Through Deep Visuali.pdf:/home/adri/Zotero/storage/IHKBZ8KB/Yosinski et al. - 2015 - Understanding Neural Networks Through Deep Visuali.pdf:application/pdf}
}
@article{yu_visualizing_2014,
	title = {Visualizing and Comparing Convolutional Neural Networks},
	url = {http://arxiv.org/abs/1412.6631},
	abstract = {Convolutional Neural Networks ({CNNs}) have achieved comparable error rates to well-trained human on {ILSVRC}2014 image classification task. To achieve better performance, the complexity of {CNNs} is continually increasing with deeper and bigger architectures. Though {CNNs} achieved promising external classification behavior, understanding of their internal work mechanism is still limited. In this work, we attempt to understand the internal work mechanism of {CNNs} by probing the internal representations in two comprehensive aspects, i.e., visualizing patches in the representation spaces constructed by different layers, and visualizing visual information kept in each layer. We further compare {CNNs} with different depths and show the advantages brought by deeper architecture.},
	journaltitle = {{arXiv}:1412.6631 [cs]},
	author = {Yu, Wei and Yang, Kuiyuan and Bai, Yalong and Yao, Hongxun and Rui, Yong},
	urldate = {2019-04-30},
	date = {2014-12-20},
	eprinttype = {arxiv},
	eprint = {1412.6631},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1412.6631 PDF:/home/adri/Zotero/storage/LWJA98K2/Yu et al. - 2014 - Visualizing and Comparing Convolutional Neural Net.pdf:application/pdf;arXiv.org Snapshot:/home/adri/Zotero/storage/V7477NZF/1412.html:text/html}
}
@article{hohman_visual_2018,
	title = {Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers},
	url = {http://arxiv.org/abs/1801.06889},
	shorttitle = {Visual Analytics in Deep Learning},
	abstract = {Deep learning has recently seen rapid development and received signiﬁcant attention due to its state-of-the-art performance on previously-thought hard problems. However, because of the internal complexity and nonlinear structure of deep neural networks, the underlying decision making processes for why these models are achieving such performance are challenging and sometimes mystifying to interpret. As deep learning spreads across domains, it is of paramount importance that we equip users of deep learning with tools for understanding when a model works correctly, when it fails, and ultimately how to improve its performance. Standardized toolkits for building neural networks have helped democratize deep learning; visual analytics systems have now been developed to support model explanation, interpretation, debugging, and improvement. We present a survey of the role of visual analytics in deep learning research, which highlights its short yet impactful history and thoroughly summarizes the state-of-the-art using a human-centered interrogative framework, focusing on the Five W’s and How (Why, Who, What, How, When, and Where). We conclude by highlighting research directions and open research problems. This survey helps researchers and practitioners in both visual analytics and deep learning to quickly learn key aspects of this young and rapidly growing body of research, whose impact spans a diverse range of domains.},
	journaltitle = {{arXiv}:1801.06889 [cs, stat]},
	author = {Hohman, Fred and Kahng, Minsuk and Pienta, Robert and Chau, Duen Horng},
	urldate = {2019-04-30},
	date = {2018-01-21},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1801.06889},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, H.5.2, I.2.6.g, I.5.1.d, I.6.9.c, I.6.9.f, Statistics - Machine Learning},
	file = {Hohman et al. - 2018 - Visual Analytics in Deep Learning An Interrogativ.pdf:/home/adri/Zotero/storage/FEUSTH29/Hohman et al. - 2018 - Visual Analytics in Deep Learning An Interrogativ.pdf:application/pdf}
}
@article{wu_prototypal_2017,
	title = {Prototypal Analysis and Prototypal Regression},
	url = {http://arxiv.org/abs/1701.08916},
	abstract = {Prototypal analysis is introduced to overcome two shortcomings of archetypal analysis: its sensitivity to outliers and its non-locality, which reduces its applicability as a learning tool. Same as archetypal analysis, prototypal analysis ﬁnds prototypes through convex combination of the data points and approximates the data through convex combination of the archetypes, but it adds a penalty for using prototypes distant from the data points for their reconstruction. Prototypal analysis can be extended—via kernel embedding—to probability distributions, since the convexity of the prototypes makes them interpretable as mixtures. Finally, prototypal regression is developed, a robust supervised procedure which allows the use of distributions as either features or labels.},
	journaltitle = {{arXiv}:1701.08916 [stat]},
	author = {Wu, Chenyue and Tabak, Esteban G.},
	urldate = {2019-03-22},
	date = {2017-01-31},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1701.08916},
	keywords = {Statistics - Machine Learning},
	file = {Wu and Tabak - 2017 - Prototypal Analysis and Prototypal Regression.pdf:/home/adri/Zotero/storage/K6TREWBA/Wu and Tabak - 2017 - Prototypal Analysis and Prototypal Regression.pdf:application/pdf}
}
@techreport{rumelhart1985learning,
  title={Learning internal representations by error propagation},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  year={1985},
  institution={California Univ San Diego La Jolla Inst for Cognitive Science}
}
@article{ribeiro_why_2016,
	title = {"{Why} {Should} {I} {Trust} {You}?": {Explaining} the {Predictions} of {Any} {Classifier}},
	shorttitle = {"{Why} {Should} {I} {Trust} {You}?},
	url = {http://arxiv.org/abs/1602.04938},
	abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
	urldate = {2019-01-08},
	journal = {arXiv:1602.04938 [cs, stat]},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	month = feb,
	year = {2016},
	note = {arXiv: 1602.04938},
	keywords = {\#bostromh, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, READ, Statistics - Machine Learning},
	file = {arXiv\:1602.04938 PDF:/home/adri/Zotero/storage/GVGLYJK6/Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf:application/pdf;arXiv.org Snapshot:/home/adri/Zotero/storage/PXHGDN98/1602.html:text/html}
}

@ARTICLE{keim_vis,
author={D. A. {Keim}},
journal={IEEE Transactions on Visualization and Computer Graphics},
title={Information visualization and visual data mining},
year={2002},
volume={8},
number={1},
pages={1-8},
keywords={data mining;data visualisation;information visualization;visual data mining;visual data exploration;data type;interaction technique;distortion technique;Data visualization;Data mining;Humans;History;Visual databases;Statistics;Machine learning;Floods;Hardware},
doi={10.1109/2945.981847},
ISSN={1077-2626},
month={Jan},}
@book{spence_information_2007,
	location = {Harlow},
	edition = {2 edition},
	title = {Information Visualization: Design for Interaction},
	isbn = {978-0-13-206550-4},
	shorttitle = {Information Visualization},
	abstract = {Up to date and easy to use, emphasising real-world examples and applications of computer-generated and interactive visualization.},
	pagetotal = {304},
	publisher = {Pearson},
	author = {Spence, Robert},
	date = {2007-01-15}
}

@ARTICLE{right_to_explanation_2016arXiv160608813G,
       author = {{Goodman}, Bryce and {Flaxman}, Seth},
        title = "{European Union regulations on algorithmic decision-making and a ''right to explanation''}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Machine Learning, Computer Science - Computers and Society, Computer Science - Machine Learning},
         year = "2016",
        month = "Jun",
          eid = {arXiv:1606.08813},
        pages = {arXiv:1606.08813},
archivePrefix = {arXiv},
       eprint = {1606.08813},
 primaryClass = {stat.ML},
       adsurl = {https://ui.adsabs.harvard.edu/\#abs/2016arXiv160608813G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{doshi-velez_towards_2017,
	title = {Towards A Rigorous Science of Interpretable Machine Learning},
	url = {http://arxiv.org/abs/1702.08608},
	abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
	journaltitle = {{arXiv}:1702.08608 [cs, stat]},
	author = {Doshi-Velez, Finale and Kim, Been},
	urldate = {2019-02-11},
	date = {2017-02-27},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1702.08608},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, \#ksankaran},
	file = {Doshi-Velez and Kim - 2017 - Towards A Rigorous Science of Interpretable Machin.pdf:/home/adri/Zotero/storage/8K7RDSU3/Doshi-Velez and Kim - 2017 - Towards A Rigorous Science of Interpretable Machin.pdf:application/pdf}
}
@article{guidotti_survey_2018,
	title = {A {Survey} {Of} {Methods} {For} {Explaining} {Black} {Box} {Models}},
	url = {http://arxiv.org/abs/1802.01933},
	abstract = {In the last years many accurate decision support systems have been constructed as black boxes, that is as systems that hide their internal logic to the user. This lack of explanation constitutes both a practical and an ethical issue. The literature reports many approaches aimed at overcoming this crucial weakness sometimes at the cost of scarifying accuracy for interpretability. The applications in which black box decision systems can be used are various, and each approach is typically developed to provide a solution for a specific problem and, as a consequence, delineating explicitly or implicitly its own definition of interpretability and explanation. The aim of this paper is to provide a classification of the main problems addressed in the literature with respect to the notion of explanation and the type of black box system. Given a problem definition, a black box type, and a desired explanation this survey should help the researcher to find the proposals more useful for his own work. The proposed classification of approaches to open black box models should also be useful for putting the many research open questions in perspective.},
	urldate = {2018-12-31},
	journal = {arXiv:1802.01933 [cs]},
	author = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Turini, Franco and Pedreschi, Dino and Giannotti, Fosca},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.01933},
	keywords = {\#bostromh, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, TO READ, Computer Science - Computers and Society, Guidotti},
	file = {arXiv\:1802.01933 PDF:/home/adri/Zotero/storage/GUQZT37V/Guidotti et al. - 2018 - A Survey Of Methods For Explaining Black Box Model.pdf:application/pdf;arXiv.org Snapshot:/home/adri/Zotero/storage/TX443DQ4/1802.html:text/html}
}
@article{kim_interpretability_2017,
	title = {Interpretability {Beyond} {Feature} {Attribution}: {Quantitative} {Testing} with {Concept} {Activation} {Vectors} ({TCAV})},
	shorttitle = {Interpretability {Beyond} {Feature} {Attribution}},
	url = {http://arxiv.org/abs/1711.11279},
	abstract = {The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level features rather than high-level concepts. To address these challenges, we introduce Concept Activation Vectors (CAVs), which provide an interpretation of a neural net's internal state in terms of human-friendly concepts. The key idea is to view the high-dimensional internal state of a neural net as an aid, not an obstacle. We show how to use CAVs as part of a technique, Testing with CAVs (TCAV), that uses directional derivatives to quantify the degree to which a user-defined concept is important to a classification result--for example, how sensitive a prediction of "zebra" is to the presence of stripes. Using the domain of image classification as a testing ground, we describe how CAVs may be used to explore hypotheses and generate insights for a standard image classification network as well as a medical application.},
	urldate = {2019-01-08},
	journal = {arXiv:1711.11279 [stat]},
	author = {Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and Sayres, Rory},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.11279},
	keywords = {Statistics - Machine Learning, \#ksankaran, concept activation vector},
	file = {arXiv\:1711.11279 PDF:/home/adri/Zotero/storage/4P2M6G9Q/Kim et al. - 2017 - Interpretability Beyond Feature Attribution Quant.pdf:application/pdf;arXiv.org Snapshot:/home/adri/Zotero/storage/AZQDTP6L/1711.html:text/html}
}
@article{tomsett_interpretable_2018,
	title = {Interpretable to {Whom}? {A} {Role}-based {Model} for {Analyzing} {Interpretable} {Machine} {Learning} {Systems}},
	shorttitle = {Interpretable to {Whom}?},
	url = {http://arxiv.org/abs/1806.07552},
	abstract = {Several researchers have argued that a machine learning system's interpretability should be defined in relation to a specific agent or task: we should not ask if the system is interpretable, but to whom is it interpretable. We describe a model intended to help answer this question, by identifying different roles that agents can fulfill in relation to the machine learning system. We illustrate the use of our model in a variety of scenarios, exploring how an agent's role influences its goals, and the implications for defining interpretability. Finally, we make suggestions for how our model could be useful to interpretability researchers, system developers, and regulatory bodies auditing machine learning systems.},
	urldate = {2018-12-31},
	journal = {arXiv:1806.07552 [cs]},
	author = {Tomsett, Richard and Braines, Dave and Harborne, Dan and Preece, Alun and Chakraborty, Supriyo},
	month = jun,
	year = {2018},
	note = {arXiv: 1806.07552},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv\:1806.07552 PDF:/home/adri/Zotero/storage/GPIGJVR3/Tomsett et al. - 2018 - Interpretable to Whom A Role-based Model for Anal.pdf:application/pdf;arXiv.org Snapshot:/home/adri/Zotero/storage/BB9TBXNC/1806.html:text/html}
}
@article{hunt_crowdsourced_2019,
	title = {Crowdsourced mapping in crisis zones: collaboration, organisation and impact},
	volume = {4},
	issn = {2364-3412, 2364-3404},
	shorttitle = {Crowdsourced mapping in crisis zones},
	url = {https://jhumanitarianaction.springeropen.com/articles/10.1186/s41018-018-0048-1},
	doi = {10.1186/s41018-018-0048-1},
	abstract = {Crowdsourced mapping has become an integral part of humanitarian response, with high profile deployments of platforms following the Haiti and Nepal earthquakes, and the multiple projects initiated during the Ebola outbreak in North West Africa in 2014, being prominent examples. There have also been hundreds of deployments of crowdsourced mapping projects across the globe that did not have a high profile. This paper, through an analysis of 51 mapping deployments between 2010 and 2016, complimented with expert interviews, seeks to explore the organisational structures that create the conditions for effective mapping actions, and the relationship between the commissioning body, often a non-governmental organisation (NGO) and the volunteers who regularly make up the team charged with producing the map. The research suggests that there are three distinct areas that need to be improved in order to provide appropriate assistance through mapping in humanitarian crisis: regionalise, prepare and research. The paper concludes, based on the case studies, how each of these areas can be handled more effectively, concluding that failure to implement one area sufficiently can lead to overall project failure.},
	language = {en},
	number = {1},
	urldate = {2019-02-15},
	journal = {Journal of International Humanitarian Action},
	author = {Hunt, Amelia and Specht, Doug},
	month = dec,
	year = {2019},
	keywords = {TO READ, crisis-response, mapping, crowdsourcing},
	file = {Hunt and Specht - 2019 - Crowdsourced mapping in crisis zones collaboratio.pdf:/home/adri/Zotero/storage/9MRGWWMX/Hunt and Specht - 2019 - Crowdsourced mapping in crisis zones collaboratio.pdf:application/pdf}
}
@article{kingma_adam:_2014,
	title = {Adam: A Method for Stochastic Optimization},
	url = {http://arxiv.org/abs/1412.6980},
	shorttitle = {Adam},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss {AdaMax}, a variant of Adam based on the infinity norm.},
	journaltitle = {{arXiv}:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	urldate = {2019-04-30},
	date = {2014-12-22},
	eprinttype = {arxiv},
	eprint = {1412.6980},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv\:1412.6980 PDF:/home/adri/Zotero/storage/H7A8R78N/Kingma and Ba - 2014 - Adam A Method for Stochastic Optimization.pdf:application/pdf;arXiv.org Snapshot:/home/adri/Zotero/storage/PJF8RJKB/1412.html:text/html}
}

@inproceedings{maggiori_can_2017,
	address = {Fort Worth, TX},
	title = {Can semantic labeling methods generalize to any city? the inria aerial image labeling benchmark},
	isbn = {978-1-5090-4951-6},
	shorttitle = {Can semantic labeling methods generalize to any city?},
	url = {http://ieeexplore.ieee.org/document/8127684/},
	doi = {10.1109/IGARSS.2017.8127684},
	abstract = {New challenges in remote sensing impose the necessity of designing pixel classiﬁcation methods that, once trained on a certain dataset, generalize to other areas of the earth. This may include regions where the appearance of the same type of objects is signiﬁcantly different. In the literature it is common to use a single image and split it into training and test sets to train a classiﬁer and assess its performance, respectively. However, this does not prove the generalization capabilities to other inputs.},
	language = {en},
	urldate = {2019-02-25},
	booktitle = {2017 {IEEE} {International} {Geoscience} and {Remote} {Sensing} {Symposium} ({IGARSS})},
	publisher = {IEEE},
	author = {Maggiori, Emmanuel and Tarabalka, Yuliya and Charpiat, Guillaume and Alliez, Pierre},
	month = jul,
	year = {2017},
	pages = {3226--3229},
	file = {Maggiori et al. - 2017 - Can semantic labeling methods generalize to any ci.pdf:/home/adri/Zotero/storage/SB3PPA5H/Maggiori et al. - 2017 - Can semantic labeling methods generalize to any ci.pdf:application/pdf}
}
@article{ronneberger_u-net:_2015,
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	shorttitle = {U-{Net}},
	url = {http://arxiv.org/abs/1505.04597},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
	urldate = {2019-02-27},
	journal = {arXiv:1505.04597 [cs]},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	month = may,
	year = {2015},
	note = {arXiv: 1505.04597},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1505.04597 PDF:/home/adri/Zotero/storage/UBPZYSAA/Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf:application/pdf;arXiv.org Snapshot:/home/adri/Zotero/storage/JQPSMWRI/1505.html:text/html}
}
